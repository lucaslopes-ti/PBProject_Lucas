## **Sprint 8 - Ingest√£o e Processamento de Dados com Spark e AWS Glue**


### **Objetivos**

Nesta sprint, o objetivo principal foi aprofundar o conhecimento sobre ingest√£o e processamento de dados utilizando Apache Spark e AWS Glue. As atividades realizadas abrangeram desde a manipula√ß√£o b√°sica de dados com Python at√© o desenvolvimento de pipelines de processamento utilizando Spark SQL e AWS Glue para armazenar os dados de forma estruturada no AWS S3.

Abaixo temos o link para acessar as pastas da sprint:

[Link para as pastas da sprint](https://github.com/Boltzmann0/PBProject_Lucas/tree/main/Sprint8)

### Atividades Realizadas

A sprint foi dividida em tr√™s grandes blocos de atividades, sendo a ultima para o desafio:

1. Manipula√ß√£o de Dados em Python
    üîπ Exerc√≠cios de manipula√ß√£o e estrutura√ß√£o de dados utilizando listas e arquivos CSV.

2. Processamento de Dados com Spark
    üîπ Utiliza√ß√£o do PySpark para transformar e enriquecer dados, aplicando consultas SQL e criando novas colunas derivadas.

3. Ingest√£o e Transforma√ß√£o de Dados com AWS Glue
    üîπ Pipeline completo de ingest√£o de arquivos CSV e JSON, transforma√ß√£o dos dados e armazenamento no AWS S3 para an√°lise com AWS Athena.

### Manipula√ß√£o de Dados em Python

Nesta etapa inicial, foram implementados dois exerc√≠cios de aquecimento (warm-up), com o objetivo de refor√ßar a manipula√ß√£o de dados em listas e arquivos CSV.
Exerc√≠cio 1 - Lista de N√∫meros Aleat√≥rios

    Criamos uma lista contendo 250 n√∫meros inteiros aleat√≥rios.
    Aplicamos a fun√ß√£o reverse() para inverter a ordem dos n√∫meros.
    O resultado foi impresso na tela.

Trecho do c√≥digo:
```
import random

numeros = [random.randint(1, 1000) for _ in range(250)]
numeros.reverse()

print(numeros)
```
Exerc√≠cio 2 - Lista de Animais e Salvamento em CSV

    Criamos uma lista contendo 20 nomes de animais.
    Ordenamos os elementos em ordem crescente.
    Armazenamos os dados em um arquivo CSV, com um nome por linha.

Trecho do c√≥digo:
```
animais = ["Cachorro", "Gato", "Elefante", "Le√£o", "Tigre", "Zebra", "Girafa", "Cavalo", "Urso", "Panda",
           "Macaco", "Lobo", "Raposa", "Coelho", "Cobra", "Papagaio", "Golfinho", "√Åguia", "Jacar√©", "Tatu"]

animais.sort()

with open("animais.csv", "w") as file:
    for animal in animais:
        file.write(f"{animal}\n")
```

### Processamento de Dados com Spark

Nesta etapa, utilizamos PySpark para manipular e enriquecer os dados de um arquivo de nomes aleat√≥rios, aplicando transforma√ß√µes, filtros e consultas SQL.

Etapas Realizadas

-Leitura do arquivo CSV com Spark.
-Adi√ß√£o das colunas Escolaridade, Pa√≠s e Ano de Nascimento com valores aleat√≥rios.
-Filtragem de registros com Spark DataFrame API e Spark SQL.
-Contagem e agrupamento de dados por gera√ß√£o e pa√≠s.

Trecho do c√≥digo:
```
df_nomes = spark.read.csv("nomes_aleatorios.txt", header=False, inferSchema=True)
df_nomes = df_nomes.withColumnRenamed("_c0", "Nomes")

df_nomes.createOrReplaceTempView("pessoas")
millennials_count = spark.sql("SELECT COUNT(*) FROM pessoas WHERE AnoNascimento BETWEEN 1980 AND 1994")
millennials_count.show()
```
Consulta SQL para agrupar os dados por gera√ß√£o:
```
SELECT Pais, 
    CASE 
        WHEN AnoNascimento BETWEEN 1944 AND 1964 THEN 'Baby Boomers'
        WHEN AnoNascimento BETWEEN 1965 AND 1979 THEN 'Gera√ß√£o X'
        WHEN AnoNascimento BETWEEN 1980 AND 1994 THEN 'Millennials'
        WHEN AnoNascimento BETWEEN 1995 AND 2015 THEN 'Gera√ß√£o Z'
        ELSE 'Outros'
    END AS Geracao,
    COUNT(*) AS Quantidade
FROM pessoas
GROUP BY Pais, Geracao
ORDER BY Pais, Geracao, Quantidade;
```

Desafio - Ingest√£o e Processamento de Dados com AWS Glue

Nesta etapa, os dados foram ingeridos e processados utilizando AWS Glue, convertendo arquivos CSV e JSON para Parquet e organizando-os na Trusted Zone.

Estrutura dos Arquivos no AWS S3
```
Dados CSV - Filmes e S√©ries
s3://data-lake-desafio/Raw/Local/CSV/Movies/
s3://data-lake-desafio/Raw/Local/CSV/Series/
```
Dados JSON - API TMDB
```
s3://data-lake-desafio/Raw/TMDB/JSON/{ano}/{mes}/{dia}/

```
3.1 Processamento de Arquivos CSV no AWS Glue

Os arquivos CSV foram processados no AWS Glue, sem particionamento, e armazenados na Trusted Zone:
```
s3://data-lake-desafio/Trusted/Local/Parquet/Movies/
s3://data-lake-desafio/Trusted/Local/Parquet/Series/
```

Trecho do c√≥digo no AWS Glue para processamento de CSV:
```
movies_df.write.mode("overwrite").parquet("s3://data-lake-desafio/Trusted/Local/Parquet/Movies/")
series_df.write.mode("overwrite").parquet("s3://data-lake-desafio/Trusted/Local/Parquet/Series/")
```
3.2 Processamento de Arquivos JSON no AWS Glue

Os arquivos JSON foram convertidos para Parquet e particionados por ano, m√™s e dia para otimizar as consultas.
```
s3://data-lake-desafio/Trusted/TMDB/Parquet/{ano}/{mes}/{dia}/
```
Trecho do c√≥digo no AWS Glue para JSON:
```
df = spark.read.option("multiline", "true").json("s3://data-lake-desafio/Raw/TMDB/JSON/*/*/*/")

df = df.withColumn("release_date", F.to_date("release_date", "yyyy-MM-dd"))
df = df.withColumn("ano", F.year("release_date"))
df = df.withColumn("mes", F.month("release_date"))
df = df.withColumn("dia", F.dayofmonth("release_date"))

df.write.mode("overwrite").partitionBy("ano", "mes", "dia").parquet("s3://data-lake-desafio/Trusted/TMDB/Parquet/")
```
Consultas e An√°lises no AWS Athena

Ap√≥s a ingest√£o e processamento dos dados, utilizamos AWS Athena para realizar consultas SQL e explorar insights.

Exemplo de consulta SQL - M√©dia de Notas por G√™nero e Ano
```
SELECT ano, genero, AVG(vote_average) AS media_nota
FROM "data_lake_trusted"
GROUP BY ano, genero
ORDER BY ano DESC;
```
Exemplo de consulta SQL - Filmes Mais Populares
```
SELECT title, popularity
FROM "data_lake_trusted"
ORDER BY popularity DESC
LIMIT 10;
```

### Evidencias

Evidencias feitas nessa sprint com os exercicios e tambem do desafio:

![evidencias](../Sprint8/evidencias/exercicios_etapa1.png)

![evidencias](../Sprint8/evidencias/evidencia_arquivoNomes.png)

![evidencias](../Sprint8/evidencias/exercicios_nomesaleatorios.png)

![evidencias](../Sprint8/evidencias/exercicios_spark.png)

![evidencias](../Sprint8/evidencias/exercicios_spark_2.png)

![evidencias](../Sprint8/evidencias/execucao_glue.png)



### Certificados

Nessa sprint nao teve cursos da AWS para a geracao de certificados.

### Conclus√£o da Sprint

Esta sprint proporcionou um aprendizado pr√°tico e estruturado sobre ingest√£o, processamento e an√°lise de dados na AWS. As etapas envolvidas inclu√≠ram desde a manipula√ß√£o b√°sica em Python at√© a constru√ß√£o de pipelines avan√ßados no AWS Glue.